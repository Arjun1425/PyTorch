{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Optuna\n",
        "\n",
        "- How to know what is the right architecture? Like: Number of nodes or hidden layers.\n",
        "- How to select learning rate, batch size, epochs, droput?\n",
        "\n",
        "Solution:\n",
        "- Experimentation. Try everything. For this Optuna comes in handy. To search the optimized hyperparameter it uses bayesian search.\n",
        "\n",
        "Things we are going to tune:\n",
        "1. Number of hidden layers.\n",
        "2. Neurons per layer.\n",
        "3. Number of epochs.\n",
        "4. Optimizer.\n",
        "5. Learning rate.\n",
        "6. Batch size.\n",
        "7. Dropout rate.\n",
        "8. weight decay (lambda).\n",
        "\n",
        "Layout for Optuna:\n",
        "- Objective function.\n",
        "  - Define search space.\n",
        "  - model init.\n",
        "  - param init.\n",
        "  - training loop.\n",
        "  - evaluation loop.\n",
        "\n",
        "- Study object."
      ],
      "metadata": {
        "id": "PUEM59r4Ha1S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rO-HM2OkCi2R"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FY_LEMb-LRBD",
        "outputId": "4629e2e9-df4f-424a-e733-a49a87ca6352"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7b10c805eb30>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBiuZO_wLVEr",
        "outputId": "250942c8-7fe6-42aa-b0a1-ca688b11b631"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3ibpHtJLmCS",
        "outputId": "3c2420fc-2d43-489f-8ad9-412ef1fe267f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_location_train = \"/content/drive/MyDrive/PyTorch/Dataset/fashion-mnist_train.csv\"\n",
        "file_location_test = \"/content/drive/MyDrive/PyTorch/Dataset/fashion-mnist_test.csv\""
      ],
      "metadata": {
        "id": "WJPkl3ZSLqoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv(file_location_train)\n",
        "test_data = pd.read_csv(file_location_test)"
      ],
      "metadata": {
        "id": "THrGqMfdLvtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = train_data.iloc[: , 1 : ].values\n",
        "y_train = train_data.iloc[:, 0].values\n",
        "X_test = test_data.iloc[ : , 1 : ].values\n",
        "y_test = test_data.iloc[ : , 0].values"
      ],
      "metadata": {
        "id": "XZV6KDWcMdRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Shape of X_train, y_train, X_test, y_test: {X_train.shape, y_train.shape, X_test.shape, y_test.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMgDUTNpMhOP",
        "outputId": "467b7dd1-952e-4271-f04b-84e36883bc1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train, y_train, X_test, y_test: ((60000, 784), (60000,), (10000, 784), (10000,))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train/255.0\n",
        "X_test = X_test/255.0"
      ],
      "metadata": {
        "id": "6saj9QC-Mks0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**First we are only going to optimize two parameters:**\n",
        "- Number of hidden layers.\n",
        "- Number of nodes per layer."
      ],
      "metadata": {
        "id": "L_UEvnETPqfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "\n",
        "  def __init__(self, feature, label):\n",
        "    self.features = feature\n",
        "    self.labels = label\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.features.shape[0]\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.features[index], self.labels[index]"
      ],
      "metadata": {
        "id": "tPGi1_hbMoAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CustomDataset(X_train, y_train)\n",
        "test_dataset = CustomDataset(X_test, y_test)"
      ],
      "metadata": {
        "id": "4DMlPil3OeyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size = 32, shuffle = True, pin_memory = True)\n",
        "test_loader = DataLoader(test_dataset, batch_size = 32, shuffle = True, pin_memory = True)"
      ],
      "metadata": {
        "id": "m68vQb26O-c7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_loader)\n",
        "print(train_dataset.features.shape[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktyCNmoAPh-Y",
        "outputId": "e4e823d8-cb9d-4e9f-d3a3-5f4af8836bdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "784\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyNN(nn.Module):\n",
        "\n",
        "  def __init__(self, input_dim, output_dim, num_hidden_layers, neurons_per_layer):\n",
        "    super().__init__()\n",
        "\n",
        "    layers = []       # Will store all the layers. Because it is going to change after every loop.\n",
        "\n",
        "    for i in range(num_hidden_layers):\n",
        "\n",
        "      layers.append(nn.Linear(input_dim, neurons_per_layer))\n",
        "      layers.append(nn.BatchNorm1d(neurons_per_layer))\n",
        "      layers.append(nn.ReLU())\n",
        "      layers.append(nn.Dropout(0.3))\n",
        "      input_dim = neurons_per_layer                            # One drawback that for all the hidden layers the number of neurons will be the same.\n",
        "\n",
        "    layers.append(nn.Linear(neurons_per_layer, output_dim))\n",
        "\n",
        "    self.model = nn.Sequential(*layers)                       # Here we are using '*' because we want to send the layers one by one not in a list all together.\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.model(x.float())"
      ],
      "metadata": {
        "id": "xSpN8EZWhDfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Objective function**"
      ],
      "metadata": {
        "id": "Yn_AcB2nP7UU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial):\n",
        "\n",
        "  # 1. next hyperparameter values from the search space.\n",
        "  num_hidden_layers = trial.suggest_int(\"num_hidden_layers\", 1, 5)\n",
        "  neurons_per_layer = trial.suggest_int(\"neurons_per_layer\", 8, 128, step = 8)\n",
        "\n",
        "  # 2. Model init\n",
        "  input_dim = train_dataset.features.shape[1]\n",
        "  output_dim = 10\n",
        "\n",
        "  model = MyNN(input_dim, output_dim, num_hidden_layers, neurons_per_layer)\n",
        "  model.to(device)\n",
        "\n",
        "  # 3. Parameter init\n",
        "  learning_rate = 0.01\n",
        "  epochs = 50\n",
        "\n",
        "  # 4. optimizer selection\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.SGD(model.parameters(), lr=0.1, weight_decay=1e-4)\n",
        "\n",
        "  # 5. Training loop\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    for batch_features, batch_labels in train_loader:\n",
        "\n",
        "      # move data to gpu\n",
        "      batch_features, batch_labels = batch_features.to(device), batch_labels.to(device)\n",
        "\n",
        "      # forward pass\n",
        "      outputs = model(batch_features)\n",
        "\n",
        "      # calculate loss\n",
        "      loss = criterion(outputs, batch_labels)\n",
        "\n",
        "      # back pass\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "\n",
        "      # update grads\n",
        "      optimizer.step()\n",
        "\n",
        "  # 6. Evaluation code\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  total = 0\n",
        "  correct = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    for batch_features, batch_labels in test_loader:\n",
        "\n",
        "      # move data to gpu\n",
        "      batch_features, batch_labels = batch_features.to(device), batch_labels.to(device)\n",
        "\n",
        "      outputs = model(batch_features)\n",
        "\n",
        "      _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "      total = total + batch_labels.shape[0]\n",
        "\n",
        "      correct = correct + (predicted == batch_labels).sum().item()\n",
        "\n",
        "    accuracy = correct/total\n",
        "\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "foCJ5Z4rPkXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nvVHj0ImnMp",
        "outputId": "82a0283d-0ebf-4ee7-eea8-5b31f11a0d22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.3.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.16.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.13.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.2)\n",
            "Downloading optuna-4.3.0-py3-none-any.whl (386 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.6/386.6 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.1-py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, alembic, optuna\n",
            "Successfully installed alembic-1.16.1 colorlog-6.9.0 optuna-4.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "\n",
        "study = optuna.create_study(direction = 'maximize')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vt8T6qswmq0R",
        "outputId": "8f50c10d-435b-4af7-ea9c-fa681e882005"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-06-06 04:59:25,263] A new study created in memory with name: no-name-2369f94d-26b5-4f5d-84dd-3563f9de5b86\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "study.optimize(objective, n_trials = 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwwB5qYkm6wJ",
        "outputId": "065da6dd-827e-4a18-83cd-760b4fcc27a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-06-06 05:10:51,989] Trial 2 finished with value: 0.8908 and parameters: {'num_hidden_layers': 1, 'neurons_per_layer': 72}. Best is trial 2 with value: 0.8908.\n",
            "[I 2025-06-06 05:16:37,737] Trial 3 finished with value: 0.8865 and parameters: {'num_hidden_layers': 5, 'neurons_per_layer': 88}. Best is trial 2 with value: 0.8908.\n",
            "[I 2025-06-06 05:22:22,925] Trial 4 finished with value: 0.8808 and parameters: {'num_hidden_layers': 5, 'neurons_per_layer': 80}. Best is trial 2 with value: 0.8908.\n",
            "[I 2025-06-06 05:25:33,562] Trial 5 finished with value: 0.8861 and parameters: {'num_hidden_layers': 1, 'neurons_per_layer': 72}. Best is trial 2 with value: 0.8908.\n",
            "[I 2025-06-06 05:29:24,107] Trial 6 finished with value: 0.8911 and parameters: {'num_hidden_layers': 2, 'neurons_per_layer': 88}. Best is trial 6 with value: 0.8911.\n",
            "[I 2025-06-06 05:32:33,516] Trial 7 finished with value: 0.8848 and parameters: {'num_hidden_layers': 1, 'neurons_per_layer': 72}. Best is trial 6 with value: 0.8911.\n",
            "[I 2025-06-06 05:37:02,517] Trial 8 finished with value: 0.8973 and parameters: {'num_hidden_layers': 3, 'neurons_per_layer': 120}. Best is trial 8 with value: 0.8973.\n",
            "[I 2025-06-06 05:40:11,905] Trial 9 finished with value: 0.8762 and parameters: {'num_hidden_layers': 1, 'neurons_per_layer': 32}. Best is trial 8 with value: 0.8973.\n",
            "[I 2025-06-06 05:46:03,939] Trial 10 finished with value: 0.8939 and parameters: {'num_hidden_layers': 5, 'neurons_per_layer': 128}. Best is trial 8 with value: 0.8973.\n",
            "[I 2025-06-06 05:51:11,457] Trial 11 finished with value: 0.5482 and parameters: {'num_hidden_layers': 4, 'neurons_per_layer': 8}. Best is trial 8 with value: 0.8973.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "study.best_value"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7LhGShTnL78",
        "outputId": "41721987-fb23-4292-bf09-72becfdee392"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8973"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "study.best_params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oJ3O8sgw0oM",
        "outputId": "b8ae51cb-f740-4e00-a317-6b552b6d1aa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'num_hidden_layers': 3, 'neurons_per_layer': 120}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now we are going to try fintune all the other hyperparameters."
      ],
      "metadata": {
        "id": "XxOc1KatZ6u-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyNN(nn.Module):\n",
        "\n",
        "  def __init__(self, input_dim, output_dim, num_hidden_layers, neurons_per_layer, dropout_rate):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    layers = []\n",
        "\n",
        "    for i in range(num_hidden_layers):\n",
        "\n",
        "      layers.append(nn.Linear(input_dim, neurons_per_layer))\n",
        "      layers.append(nn.BatchNorm1d(neurons_per_layer))\n",
        "      layers.append(nn.ReLU())\n",
        "      layers.append(nn.Dropout(dropout_rate))\n",
        "      input_dim = neurons_per_layer\n",
        "\n",
        "    layers.append(nn.Linear(neurons_per_layer, output_dim))\n",
        "\n",
        "    self.model = nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    return self.model(x.float())"
      ],
      "metadata": {
        "id": "Iq8vaCS5aReT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial):\n",
        "\n",
        "  # 1. Hyperparameter from the search.\n",
        "  num_hidden_layers = trial.suggest_int(\"num_hidden_layers\", 1, 5)\n",
        "  neurons_per_layer = trial.suggest_int(\"neurons_per_layer\", 8, 128, step = 8)\n",
        "  epochs = trial.suggest_int(\"epochs\", 10, 50, step = 10)\n",
        "  learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-1, log = True)\n",
        "  dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5, step = 0.1)\n",
        "  batch_size = trial.suggest_int(\"batch_size\", [16, 32, 64, 128])\n",
        "  optimizer_name = trial.suggest_categorical(\"optimizer\", ['Adam', 'SGD', 'RMSprop'])\n",
        "  weight_decay = trial.suggest_float(\"weight_decay\", 1e-5, 1e-3, log=True)\n",
        "\n",
        "  train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True, pin_memory = True)\n",
        "  test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle = True, pin_memory = True)\n",
        "\n",
        "  # model init\n",
        "  input_dim = 784\n",
        "  output_dim = 10\n",
        "\n",
        "  model = MyNN(input_dim, output_dim, num_hidden_layers, neurons_per_layer, dropout_rate)\n",
        "  model.to(device)\n",
        "\n",
        "  # optimizer selection\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  if optimizer_name == 'Adam':\n",
        "    optimizer = optim.Adam(model.parameters(), lr = learning_rate, weight_decay = weight_decay)\n",
        "  elif optimizer_name == 'SGD':\n",
        "    optimizer = optim.SGD(model.parameters(), lr = learning_rate, weight_decay = weight_decay)\n",
        "  else:\n",
        "    optimizer = optim.RMSprop(model.parameters(), lr = learning_rate, weight_decay = weight_decay)\n",
        "\n",
        "  # training loop\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    for batch_features, batch_labels in train_loader:\n",
        "\n",
        "      # move data to gpu\n",
        "      batch_features, batch_labels = batch_features.to(device), batch_labels.to(device)\n",
        "\n",
        "      # forward pass\n",
        "      outputs = model(batch_features)\n",
        "\n",
        "      # calculate loss\n",
        "      loss = criterion(outputs, batch_labels)\n",
        "\n",
        "      # back pass\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "\n",
        "      # update grads\n",
        "      optimizer.step()\n",
        "\n",
        "  # evaluation\n",
        "  model.eval()\n",
        "  # evaluation on test data\n",
        "  total = 0\n",
        "  correct = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    for batch_features, batch_labels in test_loader:\n",
        "\n",
        "      # move data to gpu\n",
        "      batch_features, batch_labels = batch_features.to(device), batch_labels.to(device)\n",
        "\n",
        "      outputs = model(batch_features)\n",
        "\n",
        "      _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "      total = total + batch_labels.shape[0]\n",
        "\n",
        "      correct = correct + (predicted == batch_labels).sum().item()\n",
        "\n",
        "    accuracy = correct/total\n",
        "\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "FQ_WCxmFcyTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study = optuna.create_study(direction = 'maximize')"
      ],
      "metadata": {
        "id": "1YnvVBnvkSqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study.optimize(objective, n_trials = 10)"
      ],
      "metadata": {
        "id": "olcD-BObmVHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study.best_value"
      ],
      "metadata": {
        "id": "qSyrYhLSmeV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study.best_params"
      ],
      "metadata": {
        "id": "joYe0J58mg-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How to improve NN more?**\n",
        "1. Increase number of trials.\n",
        "2. Increase the range of the hyperparameters. For eg, neurons_per_layer --> 8, 200."
      ],
      "metadata": {
        "id": "uqrnGK9vmjNH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How to improve the architecture more?**\n",
        "- Changing number of neurons_per_layer for every layer."
      ],
      "metadata": {
        "id": "s0QZeKVjnfNZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What we want to do is:**\n",
        "- Progressive shrinking/growing of layer sizes while maintaining batch normalization and dropout consistency across layers."
      ],
      "metadata": {
        "id": "i-TCXdqgr-Of"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyNN(nn.Module):\n",
        "\n",
        "  def __init__(self, input_dim, output_dim, layer_neurons, dropout_rate):\n",
        "    super().__init__()\n",
        "    layers = []\n",
        "\n",
        "    current_dim = input_dim\n",
        "\n",
        "    for neurons in layer_neurons:\n",
        "\n",
        "      layers.append(nn.Linear(current_dim, neurons))\n",
        "      layers.append(nn.BatchNorm1d(neurons))\n",
        "      layers.append(nn.ReLU())\n",
        "      layers.append(nn.Dropout(dropout_rate))\n",
        "      current_dim = neurons\n",
        "\n",
        "    layers.append(nn.Linear(current_dim, output_dim))\n",
        "    self.model = nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.model(x.float())"
      ],
      "metadata": {
        "id": "_YSPimfxoEWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial):\n",
        "\n",
        "  num_hidden_layers = trial.suggest_int(\"num_hidden_layers\", 1, 5)\n",
        "  layer_neurons = [trial.suggest_int(f\"neurons_layer_{i}\", 8, 128, step = 8) for i in range(num_hidden_layers)]\n",
        "\n",
        "  epochs = trial.suggest_int(\"epochs\", 10, 50, step = 10)\n",
        "  learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-1, log = True)\n",
        "  dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5, step = 0.1)\n",
        "  batch_size = trial.suggest_int(\"batch_size\", [16, 32, 64, 128])\n",
        "  optimizer_name = trial.suggest_categorical(\"optimizer\", ['Adam', 'SGD', 'RMSprop'])\n",
        "  weight_decay = trial.suggest_float(\"weight_decay\", 1e-5, 1e-3, log=True)"
      ],
      "metadata": {
        "id": "s_cASsHlxcYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What if I want Optuna to decide whether to use dropout and/or batch normalization, as well as the number of neurons.**"
      ],
      "metadata": {
        "id": "xRNImkg3yyu0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyNN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, layer_neurons, use_dropout, use_batchnorm, dropout_rate):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        current_dim = input_dim\n",
        "\n",
        "        for i, neurons in enumerate(layer_neurons):\n",
        "            layers.append(nn.Linear(current_dim, neurons))\n",
        "            if use_batchnorm[i]:\n",
        "                layers.append(nn.BatchNorm1d(neurons))\n",
        "            layers.append(nn.ReLU())\n",
        "            if use_dropout[i]:\n",
        "                layers.append(nn.Dropout(dropout_rate))\n",
        "            current_dim = neurons\n",
        "\n",
        "        layers.append(nn.Linear(current_dim, output_dim))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x.float())"
      ],
      "metadata": {
        "id": "_0qZfHyay_kB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial):\n",
        "    num_hidden_layers = trial.suggest_int(\"num_hidden_layers\", 1, 5)\n",
        "    layer_neurons = [\n",
        "        trial.suggest_int(f\"neurons_layer_{i}\", 8, 128, step=8)\n",
        "        for i in range(num_hidden_layers)\n",
        "    ]\n",
        "    use_dropout = [\n",
        "        trial.suggest_categorical(f\"use_dropout_{i}\", [True, False])\n",
        "        for i in range(num_hidden_layers)\n",
        "    ]\n",
        "    use_batchnorm = [\n",
        "        trial.suggest_categorical(f\"use_batchnorm_{i}\", [True, False])\n",
        "        for i in range(num_hidden_layers)\n",
        "    ]\n",
        "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5, step=0.1)\n",
        "    # ... (other hyperparameters as before)\n",
        "\n",
        "    model = MyNN(\n",
        "        input_dim=784,\n",
        "        output_dim=10,\n",
        "        layer_neurons=layer_neurons,\n",
        "        use_dropout=use_dropout,\n",
        "        use_batchnorm=use_batchnorm,\n",
        "        dropout_rate=dropout_rate\n",
        "    ).to(device)\n",
        "    # ... (rest of training/evaluation code)"
      ],
      "metadata": {
        "id": "_zABXCUyzbiX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}