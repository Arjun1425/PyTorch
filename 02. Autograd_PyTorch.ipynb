{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Why do we need Autograd?\n",
        "\n",
        "1. Find the derivative of y wrt to x in the expression y = x^2"
      ],
      "metadata": {
        "id": "1UOlXbuGiTPK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zZhfdzSTc5X-"
      },
      "outputs": [],
      "source": [
        "def dy_dx(x):\n",
        "  return 2*x       # Easy to do."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dy_dx(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6LC0S-gkb4r",
        "outputId": "093e3b99-da61-41be-cef3-076c1ce87a5f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Find the dz_dx for y = x^2 and z = sin(y)"
      ],
      "metadata": {
        "id": "KlSTURI2jyK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We can solve that by using chain rule.\n",
        "import math\n",
        "def dz_dx(x):\n",
        "  return 2 * x * math.cos(x**2)"
      ],
      "metadata": {
        "id": "YYc2hCKbkF54"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dz_dx(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWKjAhwwkefr",
        "outputId": "0ecf056a-5f99-4c89-83b4-73a5f8a9a887"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-5.466781571308061"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Let's make it more difficult. Find du_dx where, y = x^2, z = sin(y), and u = e^z. Now here we have to do the derivative three times and combine them. du_dz * dz_dy * dy_dx."
      ],
      "metadata": {
        "id": "GjOAP0eYkh_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using PyTorch\n",
        "\n",
        "import torch"
      ],
      "metadata": {
        "id": "mgjyca-ikgSA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(3.0, requires_grad=True)\n",
        "y = x**2\n",
        "print(x)\n",
        "print(y)      # PyTorch stores the way it computed y (by taking pow). Which will help it to calculate the gradient."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZGTMtOvnpue",
        "outputId": "5c685b30-eff2-4ec7-db26-a881be16bb4e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(3., requires_grad=True)\n",
            "tensor(9., grad_fn=<PowBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To do the differentiation.\n",
        "y.backward()"
      ],
      "metadata": {
        "id": "hwJFsSL9pfJ_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To see the gradient value.\n",
        "x.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4N9KV2bepmA8",
        "outputId": "9756d0c7-d94d-466a-8143-c8ccd3b6a268"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(6.)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next problem."
      ],
      "metadata": {
        "id": "BJMqWB7cp2sc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(3.0, requires_grad=True)\n",
        "y = x**2\n",
        "z = torch.sin(y)\n",
        "print(z)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vA-vFpWvpy7o",
        "outputId": "de9a31ef-4af5-4e14-aa3c-9b483ef9068b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.4121, grad_fn=<SinBackward0>)\n",
            "tensor(9., grad_fn=<PowBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z.backward()"
      ],
      "metadata": {
        "id": "DkGje7PCqSxZ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jH2Pp87QqjmN",
        "outputId": "03360e3d-2c06-4fd5-dd58-52ccb1fe4301"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-5.4668)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Want to find the gradient of a simple one node NN having activation function as sigmoid.\n",
        "--> Training on one data point only. Want to show how autograd can be used in simple NN.  "
      ],
      "metadata": {
        "id": "JFk1j1zSsbcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Inputs\n",
        "x = torch.tensor(6.7) # Input feature\n",
        "y = torch.tensor(0) # Output label\n",
        "\n",
        "# Initial guess of weights and bias.\n",
        "w = torch.tensor(1.0)\n",
        "b = torch.tensor(0.0)"
      ],
      "metadata": {
        "id": "XaFUjoQ1qlcB"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Binary Cross-Entropy loss for scalar. --> Calculating the loss.\n",
        "def binary_cross_entropy_loss(y_pred, y_true):\n",
        "  epsilon = 1e-8    # To prevent log(0)\n",
        "  y_pred = torch.clamp(y_pred, epsilon, 1-epsilon)\n",
        "  return -(y_true * torch.log(y_pred) + (1-y_true) * torch.log(1-y_pred))"
      ],
      "metadata": {
        "id": "lcTibbYG0wl9"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward pass\n",
        "z = w * x + b                          # weighted sum (linear part)\n",
        "y_pred = torch.sigmoid(z)              # activation function (predicted probability)\n",
        "\n",
        "# compute binary cross-entropy loss\n",
        "loss = binary_cross_entropy_loss(y_pred, y)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlfFjxjG2DyF",
        "outputId": "79f75142-f6b5-4adb-9a13-0a095410649a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(6.7012)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Backprop\n",
        "# 1. dL_dy_pred: Loss wrt the prediction (y_pred)\n",
        "dloss_dy_pred = (y_pred - y)/(y_pred * (1 - y_pred))\n",
        "\n",
        "# 2. dy_pred/dz: Prediction (y_pred) wrt z (sigmoid derivative)\n",
        "dy_pred_dz = y_pred * (1 - y_pred)\n",
        "\n",
        "# 3. dz/dw: z wrt w\n",
        "dz_dw = x\n",
        "\n",
        "# 4. dz/db: z wrt b\n",
        "dz_db = 1\n",
        "\n",
        "dL_dw = dloss_dy_pred * dy_pred_dz * dz_dw\n",
        "dL_db = dloss_dy_pred * dy_pred_dz * dz_db"
      ],
      "metadata": {
        "id": "1d_ivDjN2oIf"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Manual Gradient of loss dL_dw: {dL_dw}\")\n",
        "print(f\"dL_db: {dL_db}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQp0IzAT38N3",
        "outputId": "0827748e-5e0f-4d69-e508-d9ad046a9740"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Manual Gradient of loss dL_dw: 6.691762447357178\n",
            "dL_db: 0.998770534992218\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Doing same thing using autograd."
      ],
      "metadata": {
        "id": "taHlgKd74bYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(6.7)\n",
        "y = torch.tensor(0.0)\n",
        "\n",
        "w = torch.tensor(1.0, requires_grad=True)\n",
        "b = torch.tensor(0.0, requires_grad=True)"
      ],
      "metadata": {
        "id": "z5kkLivJ4JfO"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z = w * x + b\n",
        "y_pred = torch.sigmoid(z)\n",
        "loss = binary_cross_entropy_loss(y_pred, y)\n",
        "print(z, y_pred, loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkkKLZb74nge",
        "outputId": "f0cc96de-1ab3-4402-9c1f-e5afee9a5b97"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(6.7000, grad_fn=<AddBackward0>) tensor(0.9988, grad_fn=<SigmoidBackward0>) tensor(6.7012, grad_fn=<NegBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss.backward()"
      ],
      "metadata": {
        "id": "BFiYMOXp4vY8"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Autograd Gradient of loss dL_dw: {w.grad}\")\n",
        "print(f\"dL_db: {b.grad}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLdfLtFh5L8F",
        "outputId": "8aa40d59-4baa-4c3a-ea20-443b7f3685e3"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Autograd Gradient of loss dL_dw: 6.6917619705200195\n",
            "dL_db: 0.9987704753875732\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Autograd with vectors."
      ],
      "metadata": {
        "id": "HEfRff1J5Vlw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
        "\n",
        "y = x ** 2\n",
        "print(y)\n",
        "\n",
        "y = y.mean()             # grad can be implicitly created only for scalar outputs\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQNdP6T85OPC",
        "outputId": "978b9f5a-214c-443a-b651-eeb61adbfd4b"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 4., 9.], grad_fn=<PowBackward0>)\n",
            "tensor(4.6667, grad_fn=<MeanBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.backward()"
      ],
      "metadata": {
        "id": "TleiWIFR6bfF"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9P6CakkK6gfe",
        "outputId": "13eb9128-3ac4-47d2-e265-36de708bd0d6"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.6667, 1.3333, 2.0000])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clearing Gradient\n",
        "--> When we rerun the gradient we should clear the last gradient."
      ],
      "metadata": {
        "id": "3fYgC3W973Z_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(2.0, requires_grad=True)"
      ],
      "metadata": {
        "id": "3I8q-KLA7Tnc"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = x**2"
      ],
      "metadata": {
        "id": "CunESLCn9p5_"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.backward()"
      ],
      "metadata": {
        "id": "OH-OfmWk8f8C"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ha8OkV08k8s",
        "outputId": "b9239af1-ee68-4193-cfad-3c6b99a270ef"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4.)"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem: PyTorch's autograd engine accumulates gradients by default."
      ],
      "metadata": {
        "id": "5SY2IcVE-PtF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "  y = x**2\n",
        "  y.backward()\n",
        "  print(x.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeJ-OStw89rE",
        "outputId": "3436562f-5e11-4d33-8a52-035cbb080d6a"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(8.)\n",
            "tensor(12.)\n",
            "tensor(16.)\n",
            "tensor(20.)\n",
            "tensor(24.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To solve that problem we have to clear the gradient manualy. After every epochs you will have to clear gradient."
      ],
      "metadata": {
        "id": "H8IG8uQj-fMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad.zero_()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RCG10Lf-JD6",
        "outputId": "f793b0b0-ca08-4c42-d838-537056756bb9"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.)"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Disable gradient tracking.\n",
        "During prediction we don't do backprop, we only do forwardprop. Here, we can disable gradient tracking. You can do that in three ways:\n",
        "1. requires_grad_(False)\n",
        "2. detach()\n",
        "3. torch.no_grad()"
      ],
      "metadata": {
        "id": "1p6tyotK-9nO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wj6o4cx1-x5B",
        "outputId": "9489c455-8c9d-4c90-9ae5-66a5813857c2"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2., requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.requires_grad_(False)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ecn0RJvP_sLW",
        "outputId": "de89d8c2-2f73-443f-cc41-9b25767b0b12"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xf26K02_0y-",
        "outputId": "879e3462-4bc9-4f4e-9f00-cf7adfc47755"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2., requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z = x.detach()\n",
        "print(z)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJ-lgjJQATKC",
        "outputId": "63ed7edc-e2e1-4f99-8cb8-6fa16030cf07"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = x ** 2\n",
        "y.backward()\n",
        "print(x.grad)\n",
        "\n",
        "y = z ** 2\n",
        "# y.backward()               # It will give error. Beacuse you have detached the z from the computational graph\n",
        "# print(z.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWSylgQHAXWN",
        "outputId": "44af12d2-96c3-4f8b-a6e6-28ef009eecbd"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(12.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "y = x ** 2\n",
        "print(y)\n",
        "\n",
        "with torch.no_grad():\n",
        "  y = x ** 2\n",
        "  print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUU_ZkIlA5Vx",
        "outputId": "0d979f85-81a4-498a-f264-314aabfe2d9a"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(4., grad_fn=<PowBackward0>)\n",
            "tensor(4.)\n"
          ]
        }
      ]
    }
  ]
}
