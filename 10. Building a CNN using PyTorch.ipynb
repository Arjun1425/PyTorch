{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "CNN on fashion MNIST dataset.\n",
        "1. Basic CNN.\n",
        "2. Optimized CNN.\n",
        "3. Using trained CNN."
      ],
      "metadata": {
        "id": "E4HIVvwRiDYB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oy3Klrw2g_vv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F92Zl4n_i1dz",
        "outputId": "3b538c91-dfd0-407e-d3f0-ddf2fdb275c9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_location_train = \"/content/drive/MyDrive/PyTorch/Dataset/fashion-mnist_train.csv\"\n",
        "file_location_test = \"/content/drive/MyDrive/PyTorch/Dataset/fashion-mnist_test.csv\"\n",
        "train_data = pd.read_csv(file_location_train)\n",
        "test_data = pd.read_csv(file_location_test)\n",
        "X_train = train_data.iloc[: , 1 : ].values\n",
        "y_train = train_data.iloc[:, 0].values\n",
        "X_test = test_data.iloc[ : , 1 : ].values\n",
        "y_test = test_data.iloc[ : , 0].values\n",
        "print(f\"Shape of X_train, y_train, X_test, y_test: {X_train.shape, y_train.shape, X_test.shape, y_test.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1vbEWBIjDc7",
        "outputId": "571d01f2-8cce-49cb-814e-b143e57b2241"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train, y_train, X_test, y_test: ((60000, 784), (60000,), (10000, 784), (10000,))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train/255.0\n",
        "X_test = X_test/255.0"
      ],
      "metadata": {
        "id": "p_KiXYSMjWHN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "\n",
        "  def __init__(self, features, labels):\n",
        "\n",
        "    #Converting 1d input into 2d\n",
        "    self.features = torch.tensor(features, dtype = torch.float32).reshape(-1, 1, 28, 28) # Here -1 --> place holder coz we don't know our batch size yet. 1 --> number of Channels.\n",
        "    self.labels = torch.tensor(labels, dtype = torch.long)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.features)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.features[idx], self.labels[idx]"
      ],
      "metadata": {
        "id": "01LL8MU_jpxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CustomDataset(X_train, y_train)\n",
        "test_dataset = CustomDataset(X_test, y_test)\n",
        "train_loader = DataLoader(train_dataset, batch_size = 32, shuffle = True, pin_memory = True)\n",
        "test_loader = DataLoader(test_dataset, batch_size = 32, shuffle = True, pin_memory = True)"
      ],
      "metadata": {
        "id": "h29H5Uz6mPSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyNN(nn.Module):\n",
        "\n",
        "  def __init__(self, input_features):\n",
        "    super().__init__()\n",
        "\n",
        "    self.features = nn.Sequential(\n",
        "        nn.Conv2d(input_features, 32, kernel_size = 3, padding = 'same'),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
        "\n",
        "        nn.Conv2d(32, 64, kernel_size = 3, padding = 'same'),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
        "    )\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(64*7*7, 128),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(p = 0.4),\n",
        "        nn.Linear(128, 64),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(p = 0.4),\n",
        "\n",
        "        nn.Linear(64, 10),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    x = self.features(x)\n",
        "    x = self.classifier(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "gnNMaQbZm9qd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.01\n",
        "epochs = 100"
      ],
      "metadata": {
        "id": "4f37nZJFuuB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyNN(1)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1, weight_decay=1e-4)"
      ],
      "metadata": {
        "id": "i8SgjsAAu5Tx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training loop\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "  total_epoch_loss = 0\n",
        "\n",
        "  for batch_features, batch_labels in train_loader:\n",
        "\n",
        "    # move data to gpu\n",
        "    batch_features, batch_labels = batch_features.to(device), batch_labels.to(device)\n",
        "\n",
        "    # forward pass\n",
        "    outputs = model(batch_features)\n",
        "\n",
        "    # calculate loss\n",
        "    loss = criterion(outputs, batch_labels)\n",
        "\n",
        "    # back pass\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # update grads\n",
        "    optimizer.step()\n",
        "\n",
        "    total_epoch_loss = total_epoch_loss + loss.item()\n",
        "\n",
        "  avg_loss = total_epoch_loss/len(train_loader)\n",
        "  print(f'Epoch: {epoch + 1} , Loss: {avg_loss}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ylM7cAHvMbE",
        "outputId": "d8a439f6-574e-4594-a8e9-2c50867bea56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 , Loss: 0.6268563764969508\n",
            "Epoch: 2 , Loss: 0.41080416830380756\n",
            "Epoch: 3 , Loss: 0.34822434592644375\n",
            "Epoch: 4 , Loss: 0.3114048192679882\n",
            "Epoch: 5 , Loss: 0.2893591184139252\n",
            "Epoch: 6 , Loss: 0.27568318490882715\n",
            "Epoch: 7 , Loss: 0.2557085605661074\n",
            "Epoch: 8 , Loss: 0.24452517103354135\n",
            "Epoch: 9 , Loss: 0.23053308239926895\n",
            "Epoch: 10 , Loss: 0.22152272637784481\n",
            "Epoch: 11 , Loss: 0.21389741609891255\n",
            "Epoch: 12 , Loss: 0.20482621880571047\n",
            "Epoch: 13 , Loss: 0.20119932728509107\n",
            "Epoch: 14 , Loss: 0.19400162131289642\n",
            "Epoch: 15 , Loss: 0.18478751401901244\n",
            "Epoch: 16 , Loss: 0.17999691482335328\n",
            "Epoch: 17 , Loss: 0.17381538389002282\n",
            "Epoch: 18 , Loss: 0.16786805733342966\n",
            "Epoch: 19 , Loss: 0.16518814025223255\n",
            "Epoch: 20 , Loss: 0.1568270253246029\n",
            "Epoch: 21 , Loss: 0.1555611581961314\n",
            "Epoch: 22 , Loss: 0.15201763098413745\n",
            "Epoch: 23 , Loss: 0.147805726424853\n",
            "Epoch: 24 , Loss: 0.1450796476751566\n",
            "Epoch: 25 , Loss: 0.1392447899316748\n",
            "Epoch: 26 , Loss: 0.14015088228322564\n",
            "Epoch: 27 , Loss: 0.13878519051025312\n",
            "Epoch: 28 , Loss: 0.1314623478876427\n",
            "Epoch: 29 , Loss: 0.1257751117820541\n",
            "Epoch: 30 , Loss: 0.12294860700430969\n",
            "Epoch: 31 , Loss: 0.12271202827977637\n",
            "Epoch: 32 , Loss: 0.11934056452934941\n",
            "Epoch: 33 , Loss: 0.11627255236675653\n",
            "Epoch: 34 , Loss: 0.11789661317225546\n",
            "Epoch: 35 , Loss: 0.10859873937318723\n",
            "Epoch: 36 , Loss: 0.11623008387188116\n",
            "Epoch: 37 , Loss: 0.11229832150222113\n",
            "Epoch: 38 , Loss: 0.1082219646640122\n",
            "Epoch: 39 , Loss: 0.10802505529150366\n",
            "Epoch: 40 , Loss: 0.10479745828832189\n",
            "Epoch: 41 , Loss: 0.10075708464185397\n",
            "Epoch: 42 , Loss: 0.10219195520356297\n",
            "Epoch: 43 , Loss: 0.10039078514079253\n",
            "Epoch: 44 , Loss: 0.09944302566045274\n",
            "Epoch: 45 , Loss: 0.09672063308839375\n",
            "Epoch: 46 , Loss: 0.09635251658186317\n",
            "Epoch: 47 , Loss: 0.09236270313714631\n",
            "Epoch: 48 , Loss: 0.09101515981045862\n",
            "Epoch: 49 , Loss: 0.09392649042295913\n",
            "Epoch: 50 , Loss: 0.09108647047731404\n",
            "Epoch: 51 , Loss: 0.08819527567553645\n",
            "Epoch: 52 , Loss: 0.08778606718992814\n",
            "Epoch: 53 , Loss: 0.08790183968435351\n",
            "Epoch: 54 , Loss: 0.08721730424109847\n",
            "Epoch: 55 , Loss: 0.08549462764033428\n",
            "Epoch: 56 , Loss: 0.08354698200738057\n",
            "Epoch: 57 , Loss: 0.08320040563424118\n",
            "Epoch: 58 , Loss: 0.08122929552639835\n",
            "Epoch: 59 , Loss: 0.07935075215358908\n",
            "Epoch: 60 , Loss: 0.08262023138821435\n",
            "Epoch: 61 , Loss: 0.07746744142894943\n",
            "Epoch: 62 , Loss: 0.0819667474100832\n",
            "Epoch: 63 , Loss: 0.07859117864064562\n",
            "Epoch: 64 , Loss: 0.07661551807827006\n",
            "Epoch: 65 , Loss: 0.07810057019749657\n",
            "Epoch: 66 , Loss: 0.07500066949535782\n",
            "Epoch: 67 , Loss: 0.07652645181529225\n",
            "Epoch: 68 , Loss: 0.07374726982709641\n",
            "Epoch: 69 , Loss: 0.07541130002657882\n",
            "Epoch: 70 , Loss: 0.07500000217203051\n",
            "Epoch: 71 , Loss: 0.07180951496566801\n",
            "Epoch: 72 , Loss: 0.07136156705332299\n",
            "Epoch: 73 , Loss: 0.07413311177579066\n",
            "Epoch: 74 , Loss: 0.07042351010064594\n",
            "Epoch: 75 , Loss: 0.07052574463047398\n",
            "Epoch: 76 , Loss: 0.06826930025730593\n",
            "Epoch: 77 , Loss: 0.06577995363039663\n",
            "Epoch: 78 , Loss: 0.07156705727414228\n",
            "Epoch: 79 , Loss: 0.0688129669418248\n",
            "Epoch: 80 , Loss: 0.06758439341178164\n",
            "Epoch: 81 , Loss: 0.06771144162728451\n",
            "Epoch: 82 , Loss: 0.0696338063638502\n",
            "Epoch: 83 , Loss: 0.06523363491493898\n",
            "Epoch: 84 , Loss: 0.06770191981154494\n",
            "Epoch: 85 , Loss: 0.06291994810117564\n",
            "Epoch: 86 , Loss: 0.06333670484567216\n",
            "Epoch: 87 , Loss: 0.06712163563733921\n",
            "Epoch: 88 , Loss: 0.0655992413186701\n",
            "Epoch: 89 , Loss: 0.06595363804164033\n",
            "Epoch: 90 , Loss: 0.064732735425034\n",
            "Epoch: 91 , Loss: 0.06532175436178222\n",
            "Epoch: 92 , Loss: 0.06535263790981456\n",
            "Epoch: 93 , Loss: 0.06486097686503393\n",
            "Epoch: 94 , Loss: 0.0637225105583823\n",
            "Epoch: 95 , Loss: 0.062277311925403776\n",
            "Epoch: 96 , Loss: 0.06280012982290549\n",
            "Epoch: 97 , Loss: 0.06299452069486142\n",
            "Epoch: 98 , Loss: 0.060977822354342785\n",
            "Epoch: 99 , Loss: 0.06257315737575603\n",
            "Epoch: 100 , Loss: 0.059823852358587705\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z18rGZLsvR6c",
        "outputId": "709b968d-155a-4d7b-e131-13f3433fe051"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MyNN(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (6): ReLU()\n",
              "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Flatten(start_dim=1, end_dim=-1)\n",
              "    (1): Linear(in_features=3136, out_features=128, bias=True)\n",
              "    (2): ReLU()\n",
              "    (3): Dropout(p=0.4, inplace=False)\n",
              "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
              "    (5): ReLU()\n",
              "    (6): Dropout(p=0.4, inplace=False)\n",
              "    (7): Linear(in_features=64, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation on test data\n",
        "total = 0\n",
        "correct = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "  for batch_features, batch_labels in test_loader:\n",
        "\n",
        "    # move data to gpu\n",
        "    batch_features, batch_labels = batch_features.to(device), batch_labels.to(device)\n",
        "\n",
        "    outputs = model(batch_features)\n",
        "\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "    total = total + batch_labels.shape[0]\n",
        "\n",
        "    correct = correct + (predicted == batch_labels).sum().item()\n",
        "\n",
        "print(correct/total)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViFf9yT2vlJl",
        "outputId": "9df620e4-f253-4e64-aaf6-328336638b5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9273\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation on training data\n",
        "total = 0\n",
        "correct = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "  for batch_features, batch_labels in train_loader:\n",
        "\n",
        "    # move data to gpu\n",
        "    batch_features, batch_labels = batch_features.to(device), batch_labels.to(device)\n",
        "\n",
        "    outputs = model(batch_features)\n",
        "\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "    total = total + batch_labels.shape[0]\n",
        "\n",
        "    correct = correct + (predicted == batch_labels).sum().item()\n",
        "\n",
        "print(correct/total)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asEavLNdvoji",
        "outputId": "45f73bc5-1050-44d6-8019-c7112bf18626"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9956166666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Here we can see that our model is overfitting. To solve that problem we are going to do some changes in our code.\n",
        "\n",
        "1. Data Augmentation.\n",
        "2. Optuna.\n",
        "  - Number of conv. layers.\n",
        "  - Number of filters.\n",
        "  - Kernel size.\n",
        "  - Number of fully connected layer (fc).\n",
        "  - Dropout rate.\n",
        "  - Weight decay.\n",
        "  - Learning rate.\n",
        "  - optimizer name.\n",
        "  - batch_size.\n",
        "  - num_epochs."
      ],
      "metadata": {
        "id": "UZfzDnFWxIJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "# Define data augmentations for the training dataset\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.RandomHorizontalFlip(p = 0.5),\n",
        "    transforms.RandomAffine(0, translate = (0.1, 0.1)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "rmUAW36PfeFX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "\n",
        "  def __init__(self, features, labels, transform = None):\n",
        "    self.features = torch.tensor(features, dtype = torch.float32).reshape(-1, 1, 28, 28)\n",
        "    self.labels = torch.tensor(labels, dtype = torch.long)\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.features)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    feature, label = self.features[index], self.labels[index]\n",
        "    if self.transform:\n",
        "      feature =  self.transform(feature.squeeze(0).numpy())  # Transformation applied here.\n",
        "    return feature, label"
      ],
      "metadata": {
        "id": "FIkbY7xcg52H"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use augmentations in training data\n",
        "\n",
        "train_dataset = CustomDataset(X_train, y_train, transform = train_transform)\n",
        "test_dataset = CustomDataset(X_test, y_test, transform = test_transform)"
      ],
      "metadata": {
        "id": "ctCT26HhjPkz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define dynamic CNN model class\n",
        "\n",
        "class DynamicCNN(nn.Module):\n",
        "  def __init__(self, num_conv_layers, num_filters, kernel_size, num_fc_layers, fc_layer_size, dropout_rate):\n",
        "    super(DynamicCNN, self).__init__()\n",
        "    layers = []\n",
        "    in_channels = 1        # Grayscale images have 1 input channel\n",
        "\n",
        "    # Convolutional layers\n",
        "    for _ in range(num_conv_layers):\n",
        "      layers.append(nn.Conv2d(in_channels, num_filters, kernel_size = kernel_size, padding = 'same'))\n",
        "      layers.append(nn.BatchNorm2d(num_filters))\n",
        "      layers.append(nn.ReLU())\n",
        "      layers.append(nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "      in_channels = num_filters  # Update input channel for next layer\n",
        "\n",
        "    self.features = nn.Sequential(*layers)\n",
        "\n",
        "    # fully connected layers\n",
        "    fc_layers = [nn.Flatten()]\n",
        "    input_size = num_filters * (28 // (2 ** num_conv_layers)) ** 2\n",
        "    for _ in range(num_fc_layers):\n",
        "      fc_layers.append(nn.Linear(input_size, fc_layer_size))\n",
        "      fc_layers.append(nn.ReLU())\n",
        "      fc_layers.append(nn.Dropout(dropout_rate))\n",
        "      input_size = fc_layer_size\n",
        "    fc_layers.append(nn.Linear(input_size, 10))    # Final layer for 10 classes\n",
        "\n",
        "    self.classifier = nn.Sequential(*fc_layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.features(x)\n",
        "    x = self.classifier(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "Ap9jtxaykd_v"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9zBpDCvpkQw",
        "outputId": "2e2e942b-d68f-429d-cd48-6a13578bee4a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.3.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.16.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.14.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.2)\n",
            "Downloading optuna-4.3.0-py3-none-any.whl (386 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.6/386.6 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.1-py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, alembic, optuna\n",
            "Successfully installed alembic-1.16.1 colorlog-6.9.0 optuna-4.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna"
      ],
      "metadata": {
        "id": "_DfZFtGhqQ_G"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the objective function for Optuna\n",
        "def objective(trial):\n",
        "    # Hyperparameters to tune\n",
        "    num_conv_layers = trial.suggest_int('num_conv_layers', 1, 3)\n",
        "    num_filters = trial.suggest_categorical('num_filters', [16, 32, 64, 128])\n",
        "    kernel_size = trial.suggest_categorical('kernel_size', [3, 5])\n",
        "    num_fc_layers = trial.suggest_int('num_fc_layers', 1, 3)\n",
        "    fc_layer_size = trial.suggest_categorical('fc_layer_size', [64, 128, 256])\n",
        "    dropout_rate = trial.suggest_uniform('dropout_rate', 0.2, 0.5)\n",
        "    weight_decay = trial.suggest_loguniform('weight_decay', 1e-5, 1e-2)\n",
        "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
        "    optimizer_name = trial.suggest_categorical('optimizer', ['SGD', 'Adam', 'RMSprop'])\n",
        "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
        "    num_epochs = trial.suggest_int('num_epochs', 10, 30)\n",
        "\n",
        "    # Model\n",
        "    model = DynamicCNN(num_conv_layers, num_filters, kernel_size, num_fc_layers, fc_layer_size, dropout_rate).to(device)\n",
        "\n",
        "    # Data\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Optimizer\n",
        "    if optimizer_name == 'SGD':\n",
        "        optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "    elif optimizer_name == 'Adam':\n",
        "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "    elif optimizer_name == 'RMSprop':\n",
        "        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "    # Loss function\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for batch_features, batch_labels in train_loader:\n",
        "            batch_features, batch_labels = batch_features.to(device), batch_labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_features)\n",
        "            loss = criterion(outputs, batch_labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_features, batch_labels in test_loader:\n",
        "            batch_features, batch_labels = batch_features.to(device), batch_labels.to(device)\n",
        "            outputs = model(batch_features)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += batch_labels.size(0)\n",
        "            correct += (predicted == batch_labels).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "rTVOaQ_T2O98"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the Optuna study\n",
        "pruner = optuna.pruners.MedianPruner()\n",
        "study = optuna.create_study(direction='maximize', pruner=pruner)\n",
        "study.optimize(objective, n_trials=50)  # Run 50 trials"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3LcRrfnqIwY",
        "outputId": "2f044a51-747b-4782-ab0a-86b16fec99f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-06-08 22:38:02,773] A new study created in memory with name: no-name-a555c82a-c390-463b-974c-00ef918dd8f0\n",
            "<ipython-input-10-aa1058b9f892>:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.2, 0.5)\n",
            "<ipython-input-10-aa1058b9f892>:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  weight_decay = trial.suggest_loguniform('weight_decay', 1e-5, 1e-2)\n",
            "<ipython-input-10-aa1058b9f892>:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
            "[I 2025-06-08 22:50:42,488] Trial 0 finished with value: 0.8907 and parameters: {'num_conv_layers': 2, 'num_filters': 64, 'kernel_size': 3, 'num_fc_layers': 1, 'fc_layer_size': 64, 'dropout_rate': 0.37495797280676824, 'weight_decay': 0.00015167182866328903, 'learning_rate': 0.007964721148261892, 'optimizer': 'Adam', 'batch_size': 128, 'num_epochs': 30}. Best is trial 0 with value: 0.8907.\n",
            "[I 2025-06-08 23:03:37,670] Trial 1 finished with value: 0.8835 and parameters: {'num_conv_layers': 2, 'num_filters': 128, 'kernel_size': 5, 'num_fc_layers': 3, 'fc_layer_size': 256, 'dropout_rate': 0.30935648294407625, 'weight_decay': 0.00021532768391958215, 'learning_rate': 0.0025249182613693477, 'optimizer': 'RMSprop', 'batch_size': 32, 'num_epochs': 26}. Best is trial 0 with value: 0.8907.\n",
            "[I 2025-06-08 23:12:53,880] Trial 2 finished with value: 0.8901 and parameters: {'num_conv_layers': 2, 'num_filters': 16, 'kernel_size': 5, 'num_fc_layers': 2, 'fc_layer_size': 128, 'dropout_rate': 0.22277186130353707, 'weight_decay': 5.046204967536209e-05, 'learning_rate': 0.00046810151377740816, 'optimizer': 'RMSprop', 'batch_size': 32, 'num_epochs': 19}. Best is trial 0 with value: 0.8907.\n",
            "[I 2025-06-08 23:21:48,275] Trial 3 finished with value: 0.8945 and parameters: {'num_conv_layers': 2, 'num_filters': 32, 'kernel_size': 5, 'num_fc_layers': 3, 'fc_layer_size': 256, 'dropout_rate': 0.24127903783124413, 'weight_decay': 0.000957135446610091, 'learning_rate': 0.0017492568099858696, 'optimizer': 'RMSprop', 'batch_size': 64, 'num_epochs': 20}. Best is trial 3 with value: 0.8945.\n",
            "[I 2025-06-08 23:26:23,764] Trial 4 finished with value: 0.7823 and parameters: {'num_conv_layers': 2, 'num_filters': 16, 'kernel_size': 5, 'num_fc_layers': 1, 'fc_layer_size': 256, 'dropout_rate': 0.41369245469569027, 'weight_decay': 2.2487268703753848e-05, 'learning_rate': 0.0013189179156494226, 'optimizer': 'SGD', 'batch_size': 128, 'num_epochs': 11}. Best is trial 3 with value: 0.8945.\n",
            "[I 2025-06-08 23:34:32,390] Trial 5 finished with value: 0.8857 and parameters: {'num_conv_layers': 2, 'num_filters': 16, 'kernel_size': 5, 'num_fc_layers': 1, 'fc_layer_size': 128, 'dropout_rate': 0.42340349659813525, 'weight_decay': 1.953335126574016e-05, 'learning_rate': 0.006452864548276359, 'optimizer': 'SGD', 'batch_size': 32, 'num_epochs': 17}. Best is trial 3 with value: 0.8945.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the best hyperparameters and accuracy\n",
        "print(\"Best hyperparameters:\", study.best_params)\n",
        "print(\"Best accuracy:\", study.best_value)"
      ],
      "metadata": {
        "id": "qcumXAX6qkYf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}